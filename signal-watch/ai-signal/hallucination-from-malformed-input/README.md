# ğŸ§  AI Signal â€“ Hallucination from Malformed Input

Broken or incomplete signals entering LLMs or AI models can result in hallucinated outputs â€” responses that appear fluent but are factually incorrect, misleading, or unsafe.

---

## ğŸ“ Early Indicators

- Event payloads missing metadata or context  
- Ingested user actions without valid consent path  
- Prompts or structured inputs overwritten by noisy triggers  
- Model outputs contradict ground truth or embed user-specific hallucinations  

---

## ğŸ” Example Scenarios

- LLM recommends a feature that was deprecated  
- AI chatbot creates fictional user paths based on partial logs  
- Customer service model generates non-existent resolution steps  

---

## â³ RCA Escalation Path

Will be promoted to `/use-cases/ai-product/hallucination-due-to-signal-drift/` if reproducibility and system-wide impact are confirmed.

> â€œWhen the signal breaks, the story the model tells becomes fiction.â€
